{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a515ea8-1c0b-41a9-99bc-fd1f54e68c50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set database context\n",
    "spark.sql(\"USE credit_risk\")\n",
    "\n",
    "# Load the application_train table\n",
    "df = spark.sql(\"SELECT * FROM application_train\")\n",
    "\n",
    "# Display first few rows\n",
    "display(df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "386552f8-44f2-4d08-bcde-330a5246d86b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Missing Values Analysis\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import col, count, when, lit\n",
    "\n",
    "# Get total row count\n",
    "total_rows = df.count()\n",
    "\n",
    "# Calculate missing values for each column\n",
    "missing_data = []\n",
    "\n",
    "for column in df.columns:\n",
    "    # Count null values\n",
    "    null_count = df.filter(col(column).isNull()).count()\n",
    "    null_percentage = (null_count / total_rows) * 100\n",
    "    \n",
    "    # Only add if there are missing values\n",
    "    if null_count > 0:\n",
    "        missing_data.append((column, null_count, null_percentage))\n",
    "\n",
    "# Convert to DataFrame and sort by percentage\n",
    "missing_df = spark.createDataFrame(\n",
    "    missing_data,\n",
    "    [\"Column_Name\", \"Missing_Count\", \"Missing_Percentage\"]\n",
    ")\n",
    "\n",
    "# Sort by missing percentage (highest first)\n",
    "missing_df = missing_df.orderBy(col(\"Missing_Percentage\").desc())\n",
    "\n",
    "# Display results\n",
    "print(f\"=== Missing Values Report ===\")\n",
    "print(f\"Total Rows: {total_rows:,}\")\n",
    "print(f\"Columns with Missing Values: {missing_df.count()} out of {len(df.columns)}\")\n",
    "print()\n",
    "\n",
    "display(missing_df)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Optional: Create categorized view\n",
    "print(\"=== Missing Values by Severity ===\\n\")\n",
    "\n",
    "critical = missing_df.filter(col(\"Missing_Percentage\") >= 70)\n",
    "high = missing_df.filter((col(\"Missing_Percentage\") >= 40) & (col(\"Missing_Percentage\") < 70))\n",
    "medium = missing_df.filter((col(\"Missing_Percentage\") >= 10) & (col(\"Missing_Percentage\") < 40))\n",
    "low = missing_df.filter(col(\"Missing_Percentage\") < 10)\n",
    "\n",
    "print(f\"üî¥ CRITICAL (‚â•70% missing): {critical.count()} columns - RECOMMEND DROP\")\n",
    "display(critical)\n",
    "\n",
    "print(f\"\\nüü† HIGH (40-69% missing): {high.count()} columns - CONSIDER DROP OR CAREFUL IMPUTATION\")\n",
    "display(high)\n",
    "\n",
    "print(f\"\\nüü° MEDIUM (10-39% missing): {medium.count()} columns - IMPUTE\")\n",
    "display(medium)\n",
    "\n",
    "print(f\"\\nüü¢ LOW (<10% missing): {low.count()} columns - IMPUTE\")\n",
    "display(low)\n",
    "\n",
    "# COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f046c13-fd08-4efa-ab67-62030218b16b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763825029467}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC # Data Cleaning and Imputation - Complete Pipeline\n",
    "# MAGIC \n",
    "# MAGIC This notebook handles all missing values for the Home Credit Default Risk dataset:\n",
    "# MAGIC 1. Drop high missing percentage columns (>40%)\n",
    "# MAGIC 2. Impute low missing percentage columns (<10%)\n",
    "# MAGIC 3. Impute medium missing percentage columns (10-40%)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import col, when, lit\n",
    "\n",
    "# Start fresh from original data\n",
    "df_staging = df\n",
    "\n",
    "print(\"=== Starting Data Cleaning Pipeline ===\")\n",
    "print(f\"Initial shape: {df_staging.count():,} rows √ó {len(df_staging.columns)} columns\\n\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 1: Drop High Missing Percentage Columns (>40%)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# List of columns to drop (48 total: 46 building features + EXT_SOURCE_1 + OWN_CAR_AGE)\n",
    "columns_to_drop = [\n",
    "    'COMMONAREA_AVG', 'COMMONAREA_MODE', 'COMMONAREA_MEDI',\n",
    "    'NONLIVINGAPARTMENTS_AVG', 'NONLIVINGAPARTMENTS_MODE', 'NONLIVINGAPARTMENTS_MEDI',\n",
    "    'FONDKAPREMONT_MODE',\n",
    "    'LIVINGAPARTMENTS_AVG', 'LIVINGAPARTMENTS_MODE', 'LIVINGAPARTMENTS_MEDI',\n",
    "    'FLOORSMIN_AVG', 'FLOORSMIN_MODE', 'FLOORSMIN_MEDI',\n",
    "    'YEARS_BUILD_AVG', 'YEARS_BUILD_MODE', 'YEARS_BUILD_MEDI',\n",
    "    'OWN_CAR_AGE',\n",
    "    'LANDAREA_AVG', 'LANDAREA_MODE', 'LANDAREA_MEDI',\n",
    "    'BASEMENTAREA_AVG', 'BASEMENTAREA_MODE', 'BASEMENTAREA_MEDI',\n",
    "    'EXT_SOURCE_1',\n",
    "    'NONLIVINGAREA_AVG', 'NONLIVINGAREA_MODE', 'NONLIVINGAREA_MEDI',\n",
    "    'ELEVATORS_AVG', 'ELEVATORS_MODE', 'ELEVATORS_MEDI',\n",
    "    'WALLSMATERIAL_MODE',\n",
    "    'APARTMENTS_AVG', 'APARTMENTS_MODE', 'APARTMENTS_MEDI',\n",
    "    'ENTRANCES_AVG', 'ENTRANCES_MODE', 'ENTRANCES_MEDI',\n",
    "    'LIVINGAREA_AVG', 'LIVINGAREA_MODE', 'LIVINGAREA_MEDI',\n",
    "    'HOUSETYPE_MODE',\n",
    "    'FLOORSMAX_AVG', 'FLOORSMAX_MODE', 'FLOORSMAX_MEDI',\n",
    "    'YEARS_BEGINEXPLUATATION_AVG', 'YEARS_BEGINEXPLUATATION_MODE', 'YEARS_BEGINEXPLUATATION_MEDI',\n",
    "    'TOTALAREA_MODE',\n",
    "    'EMERGENCYSTATE_MODE'\n",
    "]\n",
    "\n",
    "# Drop the columns\n",
    "df_staging = df_staging.drop(*columns_to_drop)\n",
    "\n",
    "print(f\"‚úÖ Dropped {len(columns_to_drop)} columns with high missing percentages\")\n",
    "print(f\"New shape: {df_staging.count():,} rows √ó {len(df_staging.columns)} columns\\n\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 2: Calculate Medians for Numerical Columns\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Calculate medians for low missing % columns\n",
    "median_days_phone = df_staging.approxQuantile(\"DAYS_LAST_PHONE_CHANGE\", [0.5], 0.01)[0]\n",
    "median_annuity = df_staging.approxQuantile(\"AMT_ANNUITY\", [0.5], 0.01)[0]\n",
    "median_goods_price = df_staging.approxQuantile(\"AMT_GOODS_PRICE\", [0.5], 0.01)[0]\n",
    "median_ext_source_2 = df_staging.approxQuantile(\"EXT_SOURCE_2\", [0.5], 0.01)[0]\n",
    "\n",
    "# Calculate median for medium missing % column\n",
    "median_ext_source_3 = df_staging.approxQuantile(\"EXT_SOURCE_3\", [0.5], 0.01)[0]\n",
    "\n",
    "print(\"=== Calculated Medians ===\")\n",
    "print(f\"  DAYS_LAST_PHONE_CHANGE: {median_days_phone}\")\n",
    "print(f\"  AMT_ANNUITY: {median_annuity}\")\n",
    "print(f\"  AMT_GOODS_PRICE: {median_goods_price}\")\n",
    "print(f\"  EXT_SOURCE_2: {median_ext_source_2}\")\n",
    "print(f\"  EXT_SOURCE_3: {median_ext_source_3}\")\n",
    "print()\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 3: Impute Low Missing Percentage Columns (<10%)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"=== Imputing Low Missing % Columns ===\\n\")\n",
    "\n",
    "# Impute with medians\n",
    "df_staging = df_staging.withColumn(\n",
    "    \"DAYS_LAST_PHONE_CHANGE\",\n",
    "    when(col(\"DAYS_LAST_PHONE_CHANGE\").isNull(), median_days_phone)\n",
    "    .otherwise(col(\"DAYS_LAST_PHONE_CHANGE\"))\n",
    ")\n",
    "\n",
    "df_staging = df_staging.withColumn(\n",
    "    \"AMT_ANNUITY\",\n",
    "    when(col(\"AMT_ANNUITY\").isNull(), median_annuity)\n",
    "    .otherwise(col(\"AMT_ANNUITY\"))\n",
    ")\n",
    "\n",
    "df_staging = df_staging.withColumn(\n",
    "    \"AMT_GOODS_PRICE\",\n",
    "    when(col(\"AMT_GOODS_PRICE\").isNull(), median_goods_price)\n",
    "    .otherwise(col(\"AMT_GOODS_PRICE\"))\n",
    ")\n",
    "\n",
    "df_staging = df_staging.withColumn(\n",
    "    \"EXT_SOURCE_2\",\n",
    "    when(col(\"EXT_SOURCE_2\").isNull(), median_ext_source_2)\n",
    "    .otherwise(col(\"EXT_SOURCE_2\"))\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Median imputation complete for 4 columns\")\n",
    "\n",
    "# Impute CNT_FAM_MEMBERS with 1\n",
    "df_staging = df_staging.withColumn(\n",
    "    \"CNT_FAM_MEMBERS\",\n",
    "    when(col(\"CNT_FAM_MEMBERS\").isNull(), 1)\n",
    "    .otherwise(col(\"CNT_FAM_MEMBERS\"))\n",
    ")\n",
    "\n",
    "print(\"‚úÖ CNT_FAM_MEMBERS imputed with 1\")\n",
    "\n",
    "# Impute social circle columns with 0\n",
    "df_staging = df_staging.withColumn(\n",
    "    \"OBS_30_CNT_SOCIAL_CIRCLE\",\n",
    "    when(col(\"OBS_30_CNT_SOCIAL_CIRCLE\").isNull(), 0)\n",
    "    .otherwise(col(\"OBS_30_CNT_SOCIAL_CIRCLE\"))\n",
    ")\n",
    "\n",
    "df_staging = df_staging.withColumn(\n",
    "    \"DEF_30_CNT_SOCIAL_CIRCLE\",\n",
    "    when(col(\"DEF_30_CNT_SOCIAL_CIRCLE\").isNull(), 0)\n",
    "    .otherwise(col(\"DEF_30_CNT_SOCIAL_CIRCLE\"))\n",
    ")\n",
    "\n",
    "df_staging = df_staging.withColumn(\n",
    "    \"OBS_60_CNT_SOCIAL_CIRCLE\",\n",
    "    when(col(\"OBS_60_CNT_SOCIAL_CIRCLE\").isNull(), 0)\n",
    "    .otherwise(col(\"OBS_60_CNT_SOCIAL_CIRCLE\"))\n",
    ")\n",
    "\n",
    "df_staging = df_staging.withColumn(\n",
    "    \"DEF_60_CNT_SOCIAL_CIRCLE\",\n",
    "    when(col(\"DEF_60_CNT_SOCIAL_CIRCLE\").isNull(), 0)\n",
    "    .otherwise(col(\"DEF_60_CNT_SOCIAL_CIRCLE\"))\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Social circle columns imputed with 0\")\n",
    "\n",
    "# Impute NAME_TYPE_SUITE with \"Unaccompanied\"\n",
    "df_staging = df_staging.withColumn(\n",
    "    \"NAME_TYPE_SUITE\",\n",
    "    when(col(\"NAME_TYPE_SUITE\").isNull(), \"Unaccompanied\")\n",
    "    .otherwise(col(\"NAME_TYPE_SUITE\"))\n",
    ")\n",
    "\n",
    "print(\"‚úÖ NAME_TYPE_SUITE imputed with 'Unaccompanied'\")\n",
    "print(\"‚úÖ Low missing % imputation complete (10 columns)\\n\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 4: Impute Medium Missing Percentage Columns (10-40%)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"=== Imputing Medium Missing % Columns ===\\n\")\n",
    "\n",
    "# Impute OCCUPATION_TYPE with \"Unknown\"\n",
    "df_staging = df_staging.withColumn(\n",
    "    \"OCCUPATION_TYPE\",\n",
    "    when(col(\"OCCUPATION_TYPE\").isNull(), \"Unknown\")\n",
    "    .otherwise(col(\"OCCUPATION_TYPE\"))\n",
    ")\n",
    "\n",
    "print(\"‚úÖ OCCUPATION_TYPE imputed with 'Unknown'\")\n",
    "\n",
    "# Impute EXT_SOURCE_3 with median\n",
    "df_staging = df_staging.withColumn(\n",
    "    \"EXT_SOURCE_3\",\n",
    "    when(col(\"EXT_SOURCE_3\").isNull(), median_ext_source_3)\n",
    "    .otherwise(col(\"EXT_SOURCE_3\"))\n",
    ")\n",
    "\n",
    "print(\"‚úÖ EXT_SOURCE_3 imputed with median\")\n",
    "\n",
    "# Impute credit bureau inquiry columns with 0\n",
    "credit_bureau_columns = [\n",
    "    'AMT_REQ_CREDIT_BUREAU_HOUR',\n",
    "    'AMT_REQ_CREDIT_BUREAU_DAY',\n",
    "    'AMT_REQ_CREDIT_BUREAU_WEEK',\n",
    "    'AMT_REQ_CREDIT_BUREAU_MON',\n",
    "    'AMT_REQ_CREDIT_BUREAU_QRT',\n",
    "    'AMT_REQ_CREDIT_BUREAU_YEAR'\n",
    "]\n",
    "\n",
    "for col_name in credit_bureau_columns:\n",
    "    df_staging = df_staging.withColumn(\n",
    "        col_name,\n",
    "        when(col(col_name).isNull(), 0)\n",
    "        .otherwise(col(col_name))\n",
    "    )\n",
    "\n",
    "print(f\"‚úÖ Credit bureau inquiry columns imputed with 0 ({len(credit_bureau_columns)} columns)\")\n",
    "print(\"‚úÖ Medium missing % imputation complete (8 columns)\\n\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 5: Verification - Check All Imputed Columns\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# List all columns that were imputed\n",
    "all_imputed_columns = [\n",
    "    # Low missing %\n",
    "    \"DAYS_LAST_PHONE_CHANGE\",\n",
    "    \"CNT_FAM_MEMBERS\",\n",
    "    \"AMT_ANNUITY\",\n",
    "    \"AMT_GOODS_PRICE\",\n",
    "    \"EXT_SOURCE_2\",\n",
    "    \"NAME_TYPE_SUITE\",\n",
    "    \"OBS_30_CNT_SOCIAL_CIRCLE\",\n",
    "    \"DEF_30_CNT_SOCIAL_CIRCLE\",\n",
    "    \"OBS_60_CNT_SOCIAL_CIRCLE\",\n",
    "    \"DEF_60_CNT_SOCIAL_CIRCLE\",\n",
    "    # Medium missing %\n",
    "    \"OCCUPATION_TYPE\",\n",
    "    \"EXT_SOURCE_3\",\n",
    "    \"AMT_REQ_CREDIT_BUREAU_HOUR\",\n",
    "    \"AMT_REQ_CREDIT_BUREAU_DAY\",\n",
    "    \"AMT_REQ_CREDIT_BUREAU_WEEK\",\n",
    "    \"AMT_REQ_CREDIT_BUREAU_MON\",\n",
    "    \"AMT_REQ_CREDIT_BUREAU_QRT\",\n",
    "    \"AMT_REQ_CREDIT_BUREAU_YEAR\"\n",
    "]\n",
    "\n",
    "print(\"=== Verification: Missing Values After Imputation ===\")\n",
    "any_missing = False\n",
    "for column in all_imputed_columns:\n",
    "    missing_count = df_staging.filter(col(column).isNull()).count()\n",
    "    if missing_count > 0:\n",
    "        print(f\"  ‚ö†Ô∏è  {column}: {missing_count} missing values\")\n",
    "        any_missing = True\n",
    "    else:\n",
    "        print(f\"  ‚úÖ {column}: 0 missing values\")\n",
    "\n",
    "if not any_missing:\n",
    "    print(\"\\n‚úÖ All imputed columns verified - no missing values!\\n\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Warning: Some columns still have missing values\\n\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 6: Final Summary\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"=== FINAL DATA CLEANING SUMMARY ===\\n\")\n",
    "print(f\"Original dataset:\")\n",
    "print(f\"  Rows: {df.count():,}\")\n",
    "print(f\"  Columns: {len(df.columns)}\")\n",
    "\n",
    "print(f\"\\nCleaned dataset (df_staging):\")\n",
    "print(f\"  Rows: {df_staging.count():,}\")\n",
    "print(f\"  Columns: {len(df_staging.columns)}\")\n",
    "\n",
    "print(f\"\\nChanges made:\")\n",
    "print(f\"  ‚úÖ Dropped: {len(columns_to_drop)} columns (high missing %)\")\n",
    "print(f\"  ‚úÖ Imputed: {len(all_imputed_columns)} columns\")\n",
    "print(f\"     - Low missing %: 10 columns\")\n",
    "print(f\"     - Medium missing %: 8 columns\")\n",
    "\n",
    "print(f\"\\nColumn reduction: {len(df.columns)} ‚Üí {len(df_staging.columns)} ({len(df.columns) - len(df_staging.columns)} columns removed)\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 7: Check Remaining Missing Values\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Check if there are any other columns with missing values\n",
    "print(\"=== Checking for Any Remaining Missing Values ===\\n\")\n",
    "\n",
    "missing_data = []\n",
    "for column in df_staging.columns:\n",
    "    null_count = df_staging.filter(col(column).isNull()).count()\n",
    "    if null_count > 0:\n",
    "        null_pct = (null_count / df_staging.count()) * 100\n",
    "        missing_data.append((column, null_count, null_pct))\n",
    "\n",
    "if missing_data:\n",
    "    missing_df = spark.createDataFrame(missing_data, [\"Column\", \"Missing_Count\", \"Missing_Percentage\"])\n",
    "    missing_df = missing_df.orderBy(col(\"Missing_Percentage\").desc())\n",
    "    \n",
    "    print(f\"‚ö†Ô∏è  Found {len(missing_data)} columns still with missing values:\")\n",
    "    display(missing_df)\n",
    "else:\n",
    "    print(\"‚úÖ No remaining missing values in the dataset!\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 8: Save Staging Dataset\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Save the cleaned dataframe as a staging table\n",
    "df_staging.write.mode(\"overwrite\").saveAsTable(\"credit_risk.application_train_staging\")\n",
    "\n",
    "print(\"‚úÖ Cleaned dataset saved as 'credit_risk.application_train_staging'\")\n",
    "print(\"\\nüéâ Data cleaning pipeline complete!\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Display sample of cleaned data\n",
    "print(\"\\n=== Sample of Cleaned Data ===\")\n",
    "display(df_staging.limit(20))\n",
    "\n",
    "# COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f6f4ee5-b5a8-4083-81e3-18ebb2dab520",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Identify Categorical Columns and Their Values\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Load from table\n",
    "df_staging = spark.sql(\"SELECT * FROM credit_risk.application_train_staging\")\n",
    "\n",
    "print(f\"‚úÖ Data loaded: {df_staging.count():,} rows √ó {len(df_staging.columns)} columns\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import col, count, countDistinct\n",
    "\n",
    "print(\"=== Identifying Categorical Columns ===\\n\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ### Step 1: Separate Columns by Data Type\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Get column types\n",
    "numerical_cols = []\n",
    "categorical_cols = []\n",
    "\n",
    "for field in df_staging.schema.fields:\n",
    "    col_name = field.name\n",
    "    col_type = field.dataType.typeName()\n",
    "    \n",
    "    if col_type in ['string']:\n",
    "        categorical_cols.append(col_name)\n",
    "    elif col_type in ['integer', 'long', 'float', 'double']:\n",
    "        numerical_cols.append(col_name)\n",
    "\n",
    "print(f\"Total columns: {len(df_staging.columns)}\")\n",
    "print(f\"Numerical columns: {len(numerical_cols)}\")\n",
    "print(f\"Categorical columns: {len(categorical_cols)}\")\n",
    "print()\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ### Step 2: Show All Categorical Columns\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"=== CATEGORICAL COLUMNS ===\")\n",
    "for i, col_name in enumerate(categorical_cols, 1):\n",
    "    print(f\"{i}. {col_name}\")\n",
    "print()\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ### Step 3: Analyze Each Categorical Column\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"=== DETAILED ANALYSIS OF EACH CATEGORICAL COLUMN ===\\n\")\n",
    "\n",
    "categorical_summary = []\n",
    "\n",
    "for col_name in categorical_cols:\n",
    "    # Get unique value count\n",
    "    unique_count = df_staging.select(col_name).distinct().count()\n",
    "    \n",
    "    # Get total count\n",
    "    total_count = df_staging.count()\n",
    "    \n",
    "    # Store for summary\n",
    "    categorical_summary.append((col_name, unique_count, total_count))\n",
    "    \n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Column: {col_name}\")\n",
    "    print(f\"Unique Values: {unique_count}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Get value counts\n",
    "    value_counts = df_staging.groupBy(col_name).count().orderBy(col(\"count\").desc())\n",
    "    \n",
    "    # Show all values with their counts\n",
    "    print(f\"\\nValue distribution:\")\n",
    "    display(value_counts)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ### Step 4: Summary Table - Cardinality Overview\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary_df = spark.createDataFrame(\n",
    "    categorical_summary,\n",
    "    [\"Column_Name\", \"Unique_Values\", \"Total_Rows\"]\n",
    ")\n",
    "\n",
    "# Add cardinality percentage\n",
    "summary_df = summary_df.withColumn(\n",
    "    \"Cardinality_Percent\",\n",
    "    (col(\"Unique_Values\") / col(\"Total_Rows\") * 100)\n",
    ")\n",
    "\n",
    "# Sort by number of unique values\n",
    "summary_df = summary_df.orderBy(col(\"Unique_Values\").desc())\n",
    "\n",
    "print(\"=== CATEGORICAL COLUMNS SUMMARY (Sorted by Cardinality) ===\")\n",
    "print(\"Cardinality = Number of unique values\\n\")\n",
    "display(summary_df)\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ### Step 5: Categorize by Cardinality Level\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"=== COLUMNS GROUPED BY CARDINALITY ===\\n\")\n",
    "\n",
    "# Categorize columns\n",
    "low_cardinality = []      # 2-5 unique values\n",
    "medium_cardinality = []   # 6-20 unique values\n",
    "high_cardinality = []     # 20+ unique values\n",
    "\n",
    "for col_name, unique_count, _ in categorical_summary:\n",
    "    if unique_count <= 5:\n",
    "        low_cardinality.append((col_name, unique_count))\n",
    "    elif unique_count <= 20:\n",
    "        medium_cardinality.append((col_name, unique_count))\n",
    "    else:\n",
    "        high_cardinality.append((col_name, unique_count))\n",
    "\n",
    "print(f\"üü¢ LOW CARDINALITY (2-5 unique values): {len(low_cardinality)} columns\")\n",
    "print(\"   ‚Üí Easy to one-hot encode\")\n",
    "for col_name, count in sorted(low_cardinality, key=lambda x: x[1]):\n",
    "    print(f\"     ‚Ä¢ {col_name}: {count} values\")\n",
    "\n",
    "print(f\"\\nüü° MEDIUM CARDINALITY (6-20 unique values): {len(medium_cardinality)} columns\")\n",
    "print(\"   ‚Üí Good for one-hot encoding\")\n",
    "for col_name, count in sorted(medium_cardinality, key=lambda x: x[1]):\n",
    "    print(f\"     ‚Ä¢ {col_name}: {count} values\")\n",
    "\n",
    "print(f\"\\nüî¥ HIGH CARDINALITY (20+ unique values): {len(high_cardinality)} columns\")\n",
    "print(\"   ‚Üí Consider grouping rare categories or alternative encoding\")\n",
    "for col_name, count in sorted(high_cardinality, key=lambda x: x[1], reverse=True):\n",
    "    print(f\"     ‚Ä¢ {col_name}: {count} values\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ### Step 6: Check for Rare Categories (Optional)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n=== CHECKING FOR RARE CATEGORIES ===\")\n",
    "print(\"(Categories that appear in <1% of rows)\\n\")\n",
    "\n",
    "threshold = 0.01  # 1% threshold\n",
    "total_rows = df_staging.count()\n",
    "\n",
    "for col_name in categorical_cols:\n",
    "    value_counts = df_staging.groupBy(col_name).count().collect()\n",
    "    \n",
    "    rare_categories = []\n",
    "    for row in value_counts:\n",
    "        value = row[col_name]\n",
    "        count = row['count']\n",
    "        percentage = (count / total_rows) * 100\n",
    "        \n",
    "        if percentage < threshold * 100:\n",
    "            rare_categories.append((value, count, percentage))\n",
    "    \n",
    "    if rare_categories:\n",
    "        print(f\"\\n{col_name}: {len(rare_categories)} rare categories\")\n",
    "        for value, count, pct in sorted(rare_categories, key=lambda x: x[1]):\n",
    "            print(f\"  ‚Ä¢ '{value}': {count} ({pct:.2f}%)\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ### Step 7: Export Categorical Column List\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Save list of categorical columns for later use\n",
    "categorical_columns_list = categorical_cols.copy()\n",
    "\n",
    "print(\"=== CATEGORICAL COLUMNS LIST (for encoding) ===\")\n",
    "print(f\"Total: {len(categorical_columns_list)} columns\\n\")\n",
    "print(\"categorical_columns_list = [\")\n",
    "for col in categorical_columns_list:\n",
    "    print(f\"    '{col}',\")\n",
    "print(\"]\")\n",
    "\n",
    "# COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9afe4da-bd0d-4840-a407-cdec6cf53554",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Clean Up Rare and Suspicious Categorical Values\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "df_staging = spark.sql(\"SELECT * FROM credit_risk.application_train_staging\")\n",
    "\n",
    "print(\"=== Starting Categorical Cleanup ===\\n\")\n",
    "\n",
    "# Check initial row count\n",
    "initial_count = df_staging.count()\n",
    "print(f\"Initial row count: {initial_count:,}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ### Step 1: Replace CODE_GENDER \"XNA\" with \"F\"\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Check how many XNA values exist\n",
    "xna_count = df_staging.filter(col(\"CODE_GENDER\") == \"XNA\").count()\n",
    "print(f\"CODE_GENDER 'XNA' values found: {xna_count}\")\n",
    "\n",
    "# Replace XNA with F\n",
    "df_staging = df_staging.withColumn(\n",
    "    \"CODE_GENDER\",\n",
    "    when(col(\"CODE_GENDER\") == \"XNA\", \"F\")\n",
    "    .otherwise(col(\"CODE_GENDER\"))\n",
    ")\n",
    "\n",
    "# Verify the change\n",
    "print(\"\\nCODE_GENDER distribution after cleanup:\")\n",
    "df_staging.groupBy(\"CODE_GENDER\").count().orderBy(col(\"count\").desc()).show()\n",
    "\n",
    "print(\"‚úÖ CODE_GENDER: XNA replaced with F\\n\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ### Step 2: Combine Other_A and Other_B into \"Other\"\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Check current counts\n",
    "print(\"NAME_TYPE_SUITE before combining:\")\n",
    "df_staging.filter(col(\"NAME_TYPE_SUITE\").isin([\"Other_A\", \"Other_B\", \"Other\"])).groupBy(\"NAME_TYPE_SUITE\").count().show()\n",
    "\n",
    "other_a_count = df_staging.filter(col(\"NAME_TYPE_SUITE\") == \"Other_A\").count()\n",
    "other_b_count = df_staging.filter(col(\"NAME_TYPE_SUITE\") == \"Other_B\").count()\n",
    "print(f\"Other_A count: {other_a_count}\")\n",
    "print(f\"Other_B count: {other_b_count}\")\n",
    "print(f\"Combined will be: {other_a_count + other_b_count}\\n\")\n",
    "\n",
    "# Combine Other_A and Other_B into \"Other\"\n",
    "df_staging = df_staging.withColumn(\n",
    "    \"NAME_TYPE_SUITE\",\n",
    "    when(col(\"NAME_TYPE_SUITE\").isin([\"Other_A\", \"Other_B\"]), \"Other\")\n",
    "    .otherwise(col(\"NAME_TYPE_SUITE\"))\n",
    ")\n",
    "\n",
    "# Verify the change\n",
    "print(\"NAME_TYPE_SUITE distribution after combining:\")\n",
    "df_staging.groupBy(\"NAME_TYPE_SUITE\").count().orderBy(col(\"count\").desc()).show()\n",
    "\n",
    "print(\"‚úÖ NAME_TYPE_SUITE: Other_A and Other_B combined into 'Other'\\n\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ### Step 3: Drop Rows with Unknown Family Status\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Check how many Unknown values exist\n",
    "unknown_count = df_staging.filter(col(\"NAME_FAMILY_STATUS\") == \"Unknown\").count()\n",
    "print(f\"NAME_FAMILY_STATUS 'Unknown' values found: {unknown_count}\")\n",
    "\n",
    "# Drop rows where NAME_FAMILY_STATUS is \"Unknown\"\n",
    "df_staging = df_staging.filter(col(\"NAME_FAMILY_STATUS\") != \"Unknown\")\n",
    "\n",
    "# Check new row count\n",
    "final_count = df_staging.count()\n",
    "rows_dropped = initial_count - final_count\n",
    "\n",
    "print(f\"\\nRows dropped: {rows_dropped}\")\n",
    "print(f\"Final row count: {final_count:,}\")\n",
    "\n",
    "# Verify the change\n",
    "print(\"\\nNAME_FAMILY_STATUS distribution after dropping Unknown:\")\n",
    "df_staging.groupBy(\"NAME_FAMILY_STATUS\").count().orderBy(col(\"count\").desc()).show()\n",
    "\n",
    "print(\"‚úÖ NAME_FAMILY_STATUS: Unknown rows dropped\\n\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ### Step 4: Verification Summary\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"=== CLEANUP SUMMARY ===\\n\")\n",
    "print(f\"Initial rows: {initial_count:,}\")\n",
    "print(f\"Final rows: {final_count:,}\")\n",
    "print(f\"Rows removed: {rows_dropped} ({(rows_dropped/initial_count)*100:.4f}%)\")\n",
    "print()\n",
    "\n",
    "print(\"Changes made:\")\n",
    "print(\"  ‚úÖ CODE_GENDER: XNA (4 values) ‚Üí F\")\n",
    "print(\"  ‚úÖ NAME_TYPE_SUITE: Other_A + Other_B ‚Üí Other\")\n",
    "print(f\"  ‚úÖ NAME_FAMILY_STATUS: Dropped {unknown_count} rows with 'Unknown'\")\n",
    "print()\n",
    "\n",
    "# Quick check on the three columns\n",
    "print(\"Verification - Unique values in cleaned columns:\")\n",
    "print(f\"  CODE_GENDER: {df_staging.select('CODE_GENDER').distinct().count()} unique values\")\n",
    "print(f\"  NAME_TYPE_SUITE: {df_staging.select('NAME_TYPE_SUITE').distinct().count()} unique values\")\n",
    "print(f\"  NAME_FAMILY_STATUS: {df_staging.select('NAME_FAMILY_STATUS').distinct().count()} unique values\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ### Step 5: Save Cleaned Dataset\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Overwrite the staging table with cleaned data\n",
    "df_staging.write.mode(\"overwrite\").saveAsTable(\"credit_risk.application_train_staging\")\n",
    "\n",
    "print(\"‚úÖ Cleaned dataset saved back to 'credit_risk.application_train_staging'\")\n",
    "print(\"\\nüéâ Categorical cleanup complete!\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Display sample to verify\n",
    "print(\"\\n=== Sample of Cleaned Data ===\")\n",
    "display(df_staging.select(\"CODE_GENDER\", \"NAME_TYPE_SUITE\", \"NAME_FAMILY_STATUS\").limit(20))\n",
    "\n",
    "# COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4f7eeb9-6534-45e6-b826-30467790d662",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## One-Hot Encoding - Simplified Approach (Using Pandas)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=== Starting One-Hot Encoding (Pandas Approach) ===\\n\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ### Step 1: Identify Categorical Columns\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Get all categorical (string) columns\n",
    "categorical_cols = []\n",
    "\n",
    "for field in df_staging.schema.fields:\n",
    "    if field.dataType.typeName() == 'string':\n",
    "        categorical_cols.append(field.name)\n",
    "\n",
    "print(f\"Found {len(categorical_cols)} categorical columns:\\n\")\n",
    "for i, col_name in enumerate(categorical_cols, 1):\n",
    "    unique_count = df_staging.select(col_name).distinct().count()\n",
    "    print(f\"  {i}. {col_name} ({unique_count} unique values)\")\n",
    "\n",
    "print()\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ### Step 2: Convert to Pandas and One-Hot Encode\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"Converting Spark DataFrame to Pandas...\")\n",
    "print(\"(This may take 2-3 minutes for ~300K rows)\")\n",
    "\n",
    "# Convert to Pandas\n",
    "df_pandas = df_staging.toPandas()\n",
    "\n",
    "print(f\"‚úÖ Converted to Pandas: {df_pandas.shape[0]:,} rows √ó {df_pandas.shape[1]} columns\\n\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"Applying one-hot encoding...\")\n",
    "\n",
    "# One-hot encode all categorical columns at once\n",
    "df_encoded = pd.get_dummies(\n",
    "    df_pandas, \n",
    "    columns=categorical_cols,\n",
    "    drop_first=True,  # Avoid dummy variable trap\n",
    "    dtype=float  # Use float for consistency\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ One-hot encoding complete!\")\n",
    "print(f\"Final shape: {df_encoded.shape[0]:,} rows √ó {df_encoded.shape[1]} columns\")\n",
    "print(f\"Added {df_encoded.shape[1] - df_pandas.shape[1]} new columns\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ### Step 2.5: Clean Column Names for Delta Table\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"Cleaning column names for Delta table compatibility...\")\n",
    "\n",
    "# Function to clean column names\n",
    "def clean_column_name(col_name):\n",
    "    # Replace invalid characters with underscores\n",
    "    cleaned = col_name.replace(' ', '_')\n",
    "    cleaned = cleaned.replace(',', '')\n",
    "    cleaned = cleaned.replace(';', '')\n",
    "    cleaned = cleaned.replace('{', '')\n",
    "    cleaned = cleaned.replace('}', '')\n",
    "    cleaned = cleaned.replace('(', '')\n",
    "    cleaned = cleaned.replace(')', '')\n",
    "    cleaned = cleaned.replace('\\n', '')\n",
    "    cleaned = cleaned.replace('\\t', '')\n",
    "    cleaned = cleaned.replace('=', '')\n",
    "    cleaned = cleaned.replace('/', '_')\n",
    "    cleaned = cleaned.replace(':', '_')\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "# Rename all columns\n",
    "df_encoded.columns = [clean_column_name(col) for col in df_encoded.columns]\n",
    "\n",
    "print(f\"‚úÖ Column names cleaned!\")\n",
    "print(f\"\\nSample of cleaned column names:\")\n",
    "for col in df_encoded.columns[:20]:\n",
    "    print(f\"  ‚Ä¢ {col}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ### Step 3: Verification\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n=== ENCODING VERIFICATION ===\\n\")\n",
    "\n",
    "# Check data types\n",
    "print(\"Data types in encoded dataset:\")\n",
    "print(df_encoded.dtypes.value_counts())\n",
    "print()\n",
    "\n",
    "# Check for missing values\n",
    "missing_count = df_encoded.isnull().sum().sum()\n",
    "print(f\"Total missing values: {missing_count}\")\n",
    "\n",
    "if missing_count > 0:\n",
    "    print(\"\\nColumns with missing values:\")\n",
    "    missing_cols = df_encoded.isnull().sum()\n",
    "    print(missing_cols[missing_cols > 0])\n",
    "else:\n",
    "    print(\"‚úÖ No missing values!\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Show sample\n",
    "print(\"Sample of encoded data (first 5 rows, first 10 columns):\")\n",
    "display(df_encoded.head().iloc[:, :10])\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ### Step 4: Convert Back to Spark and Save\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"Converting back to Spark DataFrame...\")\n",
    "\n",
    "# Convert Pandas back to Spark\n",
    "df_final_spark = spark.createDataFrame(df_encoded)\n",
    "\n",
    "print(f\"‚úÖ Converted back to Spark\")\n",
    "\n",
    "# Save to table\n",
    "print(\"Saving to table...\")\n",
    "df_final_spark.write.mode(\"overwrite\").saveAsTable(\"credit_risk.application_train_encoded\")\n",
    "\n",
    "print(\"‚úÖ Encoded dataset saved as 'credit_risk.application_train_encoded'\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ### Step 5: Summary\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n=== FINAL SUMMARY ===\\n\")\n",
    "print(f\"Original columns: {len(df_staging.columns)}\")\n",
    "print(f\"Final columns: {df_encoded.shape[1]}\")\n",
    "print(f\"Net change: +{df_encoded.shape[1] - len(df_staging.columns)} columns\")\n",
    "print()\n",
    "print(f\"Original categorical columns: {len(categorical_cols)}\")\n",
    "print(f\"New binary columns created: {df_encoded.shape[1] - len(df_staging.columns) + len(categorical_cols)}\")\n",
    "print()\n",
    "print(\"‚úÖ Dataset is now fully numerical and ready for:\")\n",
    "print(\"  ‚Ä¢ Feature scaling\")\n",
    "print(\"  ‚Ä¢ Correlation analysis\")\n",
    "print(\"  ‚Ä¢ Logistic regression modeling\")\n",
    "print()\n",
    "print(\"üéâ One-Hot Encoding Complete!\")\n",
    "\n",
    "# COMMAND ----------"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data_wrangling",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
