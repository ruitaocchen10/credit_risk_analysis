{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a515ea8-1c0b-41a9-99bc-fd1f54e68c50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set database context\n",
    "spark.sql(\"USE credit_risk\")\n",
    "\n",
    "# Load the application_train table\n",
    "df = spark.sql(\"SELECT * FROM application_train\")\n",
    "\n",
    "# Display first few rows\n",
    "display(df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "386552f8-44f2-4d08-bcde-330a5246d86b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Missing Values Analysis\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import col, count, when, lit\n",
    "\n",
    "# Get total row count\n",
    "total_rows = df.count()\n",
    "\n",
    "# Calculate missing values for each column\n",
    "missing_data = []\n",
    "\n",
    "for column in df.columns:\n",
    "    # Count null values\n",
    "    null_count = df.filter(col(column).isNull()).count()\n",
    "    null_percentage = (null_count / total_rows) * 100\n",
    "    \n",
    "    # Only add if there are missing values\n",
    "    if null_count > 0:\n",
    "        missing_data.append((column, null_count, null_percentage))\n",
    "\n",
    "# Convert to DataFrame and sort by percentage\n",
    "missing_df = spark.createDataFrame(\n",
    "    missing_data,\n",
    "    [\"Column_Name\", \"Missing_Count\", \"Missing_Percentage\"]\n",
    ")\n",
    "\n",
    "# Sort by missing percentage (highest first)\n",
    "missing_df = missing_df.orderBy(col(\"Missing_Percentage\").desc())\n",
    "\n",
    "# Display results\n",
    "print(f\"=== Missing Values Report ===\")\n",
    "print(f\"Total Rows: {total_rows:,}\")\n",
    "print(f\"Columns with Missing Values: {missing_df.count()} out of {len(df.columns)}\")\n",
    "print()\n",
    "\n",
    "display(missing_df)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Optional: Create categorized view\n",
    "print(\"=== Missing Values by Severity ===\\n\")\n",
    "\n",
    "critical = missing_df.filter(col(\"Missing_Percentage\") >= 70)\n",
    "high = missing_df.filter((col(\"Missing_Percentage\") >= 40) & (col(\"Missing_Percentage\") < 70))\n",
    "medium = missing_df.filter((col(\"Missing_Percentage\") >= 10) & (col(\"Missing_Percentage\") < 40))\n",
    "low = missing_df.filter(col(\"Missing_Percentage\") < 10)\n",
    "\n",
    "print(f\"ðŸ”´ CRITICAL (â‰¥70% missing): {critical.count()} columns - RECOMMEND DROP\")\n",
    "display(critical)\n",
    "\n",
    "print(f\"\\nðŸŸ  HIGH (40-69% missing): {high.count()} columns - CONSIDER DROP OR CAREFUL IMPUTATION\")\n",
    "display(high)\n",
    "\n",
    "print(f\"\\nðŸŸ¡ MEDIUM (10-39% missing): {medium.count()} columns - IMPUTE\")\n",
    "display(medium)\n",
    "\n",
    "print(f\"\\nðŸŸ¢ LOW (<10% missing): {low.count()} columns - IMPUTE\")\n",
    "display(low)\n",
    "\n",
    "# COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f046c13-fd08-4efa-ab67-62030218b16b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Impute Missing Values - Low Missing Percentage Columns (<10%)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import col, when, lit\n",
    "\n",
    "# Make a copy to work with\n",
    "df_imputed = df\n",
    "\n",
    "print(\"=== Starting Imputation Process ===\\n\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Step 1: Calculate Medians for Numerical Columns\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Calculate medians (using approxQuantile for better performance)\n",
    "median_days_phone = df.approxQuantile(\"DAYS_LAST_PHONE_CHANGE\", [0.5], 0.01)[0]\n",
    "median_annuity = df.approxQuantile(\"AMT_ANNUITY\", [0.5], 0.01)[0]\n",
    "median_goods_price = df.approxQuantile(\"AMT_GOODS_PRICE\", [0.5], 0.01)[0]\n",
    "median_ext_source_2 = df.approxQuantile(\"EXT_SOURCE_2\", [0.5], 0.01)[0]\n",
    "\n",
    "print(\"Calculated Medians:\")\n",
    "print(f\"  DAYS_LAST_PHONE_CHANGE: {median_days_phone}\")\n",
    "print(f\"  AMT_ANNUITY: {median_annuity}\")\n",
    "print(f\"  AMT_GOODS_PRICE: {median_goods_price}\")\n",
    "print(f\"  EXT_SOURCE_2: {median_ext_source_2}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Step 2: Impute with Medians\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Impute DAYS_LAST_PHONE_CHANGE with median\n",
    "df_imputed = df_imputed.withColumn(\n",
    "    \"DAYS_LAST_PHONE_CHANGE\",\n",
    "    when(col(\"DAYS_LAST_PHONE_CHANGE\").isNull(), median_days_phone)\n",
    "    .otherwise(col(\"DAYS_LAST_PHONE_CHANGE\"))\n",
    ")\n",
    "\n",
    "# Impute AMT_ANNUITY with median\n",
    "df_imputed = df_imputed.withColumn(\n",
    "    \"AMT_ANNUITY\",\n",
    "    when(col(\"AMT_ANNUITY\").isNull(), median_annuity)\n",
    "    .otherwise(col(\"AMT_ANNUITY\"))\n",
    ")\n",
    "\n",
    "# Impute AMT_GOODS_PRICE with median\n",
    "df_imputed = df_imputed.withColumn(\n",
    "    \"AMT_GOODS_PRICE\",\n",
    "    when(col(\"AMT_GOODS_PRICE\").isNull(), median_goods_price)\n",
    "    .otherwise(col(\"AMT_GOODS_PRICE\"))\n",
    ")\n",
    "\n",
    "# Impute EXT_SOURCE_2 with median\n",
    "df_imputed = df_imputed.withColumn(\n",
    "    \"EXT_SOURCE_2\",\n",
    "    when(col(\"EXT_SOURCE_2\").isNull(), median_ext_source_2)\n",
    "    .otherwise(col(\"EXT_SOURCE_2\"))\n",
    ")\n",
    "\n",
    "print(\"âœ… Median imputation complete for 4 columns\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Step 3: Impute with Direct Values\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Impute CNT_FAM_MEMBERS with 1\n",
    "df_imputed = df_imputed.withColumn(\n",
    "    \"CNT_FAM_MEMBERS\",\n",
    "    when(col(\"CNT_FAM_MEMBERS\").isNull(), 1)\n",
    "    .otherwise(col(\"CNT_FAM_MEMBERS\"))\n",
    ")\n",
    "\n",
    "# Impute social circle columns with 0\n",
    "df_imputed = df_imputed.withColumn(\n",
    "    \"OBS_30_CNT_SOCIAL_CIRCLE\",\n",
    "    when(col(\"OBS_30_CNT_SOCIAL_CIRCLE\").isNull(), 0)\n",
    "    .otherwise(col(\"OBS_30_CNT_SOCIAL_CIRCLE\"))\n",
    ")\n",
    "\n",
    "df_imputed = df_imputed.withColumn(\n",
    "    \"DEF_30_CNT_SOCIAL_CIRCLE\",\n",
    "    when(col(\"DEF_30_CNT_SOCIAL_CIRCLE\").isNull(), 0)\n",
    "    .otherwise(col(\"DEF_30_CNT_SOCIAL_CIRCLE\"))\n",
    ")\n",
    "\n",
    "df_imputed = df_imputed.withColumn(\n",
    "    \"OBS_60_CNT_SOCIAL_CIRCLE\",\n",
    "    when(col(\"OBS_60_CNT_SOCIAL_CIRCLE\").isNull(), 0)\n",
    "    .otherwise(col(\"OBS_60_CNT_SOCIAL_CIRCLE\"))\n",
    ")\n",
    "\n",
    "df_imputed = df_imputed.withColumn(\n",
    "    \"DEF_60_CNT_SOCIAL_CIRCLE\",\n",
    "    when(col(\"DEF_60_CNT_SOCIAL_CIRCLE\").isNull(), 0)\n",
    "    .otherwise(col(\"DEF_60_CNT_SOCIAL_CIRCLE\"))\n",
    ")\n",
    "\n",
    "print(\"âœ… Direct value imputation complete for 5 columns\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Step 4: Impute Categorical Column\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Impute NAME_TYPE_SUITE with \"Unaccompanied\"\n",
    "df_imputed = df_imputed.withColumn(\n",
    "    \"NAME_TYPE_SUITE\",\n",
    "    when(col(\"NAME_TYPE_SUITE\").isNull(), \"Unaccompanied\")\n",
    "    .otherwise(col(\"NAME_TYPE_SUITE\"))\n",
    ")\n",
    "\n",
    "print(\"âœ… Categorical imputation complete for 1 column\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Step 5: Verify Imputation Results\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Check that these columns now have no missing values\n",
    "columns_imputed = [\n",
    "    \"DAYS_LAST_PHONE_CHANGE\",\n",
    "    \"CNT_FAM_MEMBERS\",\n",
    "    \"AMT_ANNUITY\",\n",
    "    \"AMT_GOODS_PRICE\",\n",
    "    \"EXT_SOURCE_2\",\n",
    "    \"NAME_TYPE_SUITE\",\n",
    "    \"OBS_30_CNT_SOCIAL_CIRCLE\",\n",
    "    \"DEF_30_CNT_SOCIAL_CIRCLE\",\n",
    "    \"OBS_60_CNT_SOCIAL_CIRCLE\",\n",
    "    \"DEF_60_CNT_SOCIAL_CIRCLE\"\n",
    "]\n",
    "\n",
    "print(\"\\n=== Verification: Missing Values After Imputation ===\")\n",
    "for column in columns_imputed:\n",
    "    missing_count = df_imputed.filter(col(column).isNull()).count()\n",
    "    print(f\"  {column}: {missing_count} missing values\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Show before/after comparison\n",
    "print(\"\\n=== Summary ===\")\n",
    "print(f\"Total rows: {df_imputed.count():,}\")\n",
    "print(f\"Total columns: {len(df_imputed.columns)}\")\n",
    "print(f\"\\nâœ… Successfully imputed 10 columns with low missing percentages!\")\n",
    "\n",
    "# Display sample of imputed data\n",
    "display(df_imputed.select(columns_imputed).limit(20))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# MAGIC %md\n",
    "# MAGIC ### Step 6: Save Imputed Dataset (Optional)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Optional: Save the imputed dataframe as a new table\n",
    "df_imputed.write.mode(\"overwrite\").saveAsTable(\"credit_risk.application_train_imputed\")\n",
    "\n",
    "print(\"âœ… Imputed dataset saved as 'credit_risk.application_train_imputed'\")\n",
    "\n",
    "# COMMAND ----------"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "missing_values",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
