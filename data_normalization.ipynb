{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44201cea-4a20-4f62-b2d4-724e1c07977d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC # Fix Numerical Values - DAYS_EMPLOYED Anomaly\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Load the encoded data\n",
    "spark.sql(\"USE credit_risk\")\n",
    "df_encoded = spark.table(\"application_train_encoded\")\n",
    "\n",
    "print(f\"Loaded data: {df_encoded.count():,} rows √ó {len(df_encoded.columns)} columns\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 1: Verify the DAYS_EMPLOYED Anomaly\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Check how many rows have the anomaly\n",
    "anomaly_count = df_encoded.filter(col(\"DAYS_EMPLOYED\") == 365243).count()\n",
    "total_count = df_encoded.count()\n",
    "anomaly_pct = (anomaly_count / total_count) * 100\n",
    "\n",
    "print(f\"=== DAYS_EMPLOYED Anomaly Check ===\")\n",
    "print(f\"Total rows: {total_count:,}\")\n",
    "print(f\"Rows with DAYS_EMPLOYED = 365243: {anomaly_count:,}\")\n",
    "print(f\"Percentage: {anomaly_pct:.2f}%\")\n",
    "print()\n",
    "\n",
    "# Show distribution before fix\n",
    "print(\"DAYS_EMPLOYED distribution (top 10 values):\")\n",
    "df_encoded.groupBy(\"DAYS_EMPLOYED\").count().orderBy(col(\"count\").desc()).show(10)\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 2: Create Unemployment Flag\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"Creating IS_UNEMPLOYED flag...\")\n",
    "\n",
    "# Create binary flag: 1 if unemployed (365243), 0 if employed\n",
    "df_fixed = df_encoded.withColumn(\n",
    "    \"IS_UNEMPLOYED\",\n",
    "    when(col(\"DAYS_EMPLOYED\") == 365243, 1.0).otherwise(0.0)\n",
    ")\n",
    "\n",
    "# Verify flag creation\n",
    "unemployed_flag_count = df_fixed.filter(col(\"IS_UNEMPLOYED\") == 1.0).count()\n",
    "print(f\"‚úÖ IS_UNEMPLOYED flag created\")\n",
    "print(f\"   Unemployed (IS_UNEMPLOYED = 1): {unemployed_flag_count:,}\")\n",
    "print(f\"   Employed (IS_UNEMPLOYED = 0): {total_count - unemployed_flag_count:,}\")\n",
    "print()\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 3: Replace 365243 with 0\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"Replacing DAYS_EMPLOYED = 365243 with 0...\")\n",
    "\n",
    "# Replace the anomaly with 0\n",
    "df_fixed = df_fixed.withColumn(\n",
    "    \"DAYS_EMPLOYED\",\n",
    "    when(col(\"DAYS_EMPLOYED\") == 365243, 0).otherwise(col(\"DAYS_EMPLOYED\"))\n",
    ")\n",
    "\n",
    "print(\"‚úÖ DAYS_EMPLOYED anomaly replaced with 0\")\n",
    "print()\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 4: Verify the Fix\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"=== Verification ===\\n\")\n",
    "\n",
    "# Check that 365243 is gone\n",
    "anomaly_after = df_fixed.filter(col(\"DAYS_EMPLOYED\") == 365243).count()\n",
    "print(f\"Rows with DAYS_EMPLOYED = 365243 after fix: {anomaly_after}\")\n",
    "\n",
    "# Check that zeros were added\n",
    "zeros_count = df_fixed.filter(col(\"DAYS_EMPLOYED\") == 0).count()\n",
    "print(f\"Rows with DAYS_EMPLOYED = 0: {zeros_count:,}\")\n",
    "\n",
    "# Check flag matches zeros\n",
    "print(f\"IS_UNEMPLOYED = 1 count: {df_fixed.filter(col('IS_UNEMPLOYED') == 1.0).count():,}\")\n",
    "print(f\"Match: {zeros_count == df_fixed.filter(col('IS_UNEMPLOYED') == 1.0).count()}\")\n",
    "print()\n",
    "\n",
    "# Show new distribution\n",
    "print(\"DAYS_EMPLOYED distribution after fix (top 10 values):\")\n",
    "df_fixed.groupBy(\"DAYS_EMPLOYED\").count().orderBy(col(\"count\").desc()).show(10)\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 5: Check Statistics\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Compare statistics before and after\n",
    "print(\"=== DAYS_EMPLOYED Statistics ===\\n\")\n",
    "\n",
    "# After fix (excluding zeros to see employed distribution)\n",
    "employed_only = df_fixed.filter(col(\"DAYS_EMPLOYED\") != 0)\n",
    "\n",
    "stats = df_fixed.select(\n",
    "    col(\"DAYS_EMPLOYED\")\n",
    ").summary(\"min\", \"25%\", \"50%\", \"75%\", \"max\", \"mean\", \"stddev\")\n",
    "\n",
    "print(\"Statistics (including unemployed = 0):\")\n",
    "stats.show()\n",
    "\n",
    "print(\"\\nStatistics (employed only, excluding zeros):\")\n",
    "employed_only.select(\"DAYS_EMPLOYED\").summary(\"min\", \"25%\", \"50%\", \"75%\", \"max\", \"mean\").show()\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 6: Verify Column Count\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"=== Column Count ===\")\n",
    "print(f\"Original columns: {len(df_encoded.columns)}\")\n",
    "print(f\"After adding IS_UNEMPLOYED: {len(df_fixed.columns)}\")\n",
    "print(f\"New columns added: {len(df_fixed.columns) - len(df_encoded.columns)}\")\n",
    "print()\n",
    "\n",
    "# Verify IS_UNEMPLOYED is in the dataframe\n",
    "if \"IS_UNEMPLOYED\" in df_fixed.columns:\n",
    "    print(\"‚úÖ IS_UNEMPLOYED column successfully added\")\n",
    "else:\n",
    "    print(\"‚ùå ERROR: IS_UNEMPLOYED column not found\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Step 7: Save Fixed Dataset\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Save the fixed dataset\n",
    "df_fixed.write.mode(\"overwrite\").saveAsTable(\"credit_risk.application_train_fixed\")\n",
    "\n",
    "print(\"‚úÖ Fixed dataset saved as 'credit_risk.application_train_fixed'\")\n",
    "print()\n",
    "print(\"=== Summary ===\")\n",
    "print(f\"‚Ä¢ Fixed DAYS_EMPLOYED anomaly (365243 ‚Üí 0)\")\n",
    "print(f\"‚Ä¢ Created IS_UNEMPLOYED flag\")\n",
    "print(f\"‚Ä¢ Affected rows: {anomaly_count:,} ({anomaly_pct:.2f}%)\")\n",
    "print(f\"‚Ä¢ Total rows: {df_fixed.count():,}\")\n",
    "print(f\"‚Ä¢ Total columns: {len(df_fixed.columns)}\")\n",
    "print()\n",
    "print(\"üéâ Ready for feature scaling!\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Display sample to verify\n",
    "print(\"Sample of fixed data:\")\n",
    "display(df_fixed.select(\"SK_ID_CURR\", \"DAYS_EMPLOYED\", \"IS_UNEMPLOYED\", \"TARGET\").limit(20))\n",
    "\n",
    "# COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13428dec-da37-4459-bdde-9ce051e73454",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ## Apply StandardScaler to Numerical Features\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"=== Starting Feature Scaling ===\\n\")\n",
    "\n",
    "# Load the fixed data\n",
    "spark.sql(\"USE credit_risk\")\n",
    "df_fixed = spark.table(\"application_train_fixed\")\n",
    "\n",
    "print(f\"Loaded data: {df_fixed.count():,} rows √ó {len(df_fixed.columns)} columns\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ### Step 1: Identify Columns to Scale vs Not Scale\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Get all column names and types\n",
    "all_columns = df_fixed.columns\n",
    "\n",
    "# Columns to EXCLUDE from scaling\n",
    "columns_to_exclude = [\n",
    "    'SK_ID_CURR',  # ID column - don't scale\n",
    "    'TARGET'       # Target variable - don't scale\n",
    "]\n",
    "\n",
    "# Binary/one-hot encoded columns (0/1 values) - don't scale these either\n",
    "# These are columns created from categorical encoding\n",
    "binary_columns = [col for col in all_columns if col not in columns_to_exclude and \n",
    "                  df_fixed.select(col).distinct().count() == 2]\n",
    "\n",
    "print(f\"Total columns: {len(all_columns)}\")\n",
    "print(f\"Binary/one-hot encoded columns (will NOT scale): {len(binary_columns)}\")\n",
    "print(f\"\\nFirst 20 binary columns:\")\n",
    "for col_name in binary_columns[:20]:\n",
    "    print(f\"  ‚Ä¢ {col_name}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Numerical columns to scale\n",
    "# These are columns that are NOT binary and NOT excluded\n",
    "numerical_columns_to_scale = [\n",
    "    col for col in all_columns \n",
    "    if col not in columns_to_exclude and col not in binary_columns\n",
    "]\n",
    "\n",
    "print(f\"\\n=== Columns to Scale ===\")\n",
    "print(f\"Total numerical columns to scale: {len(numerical_columns_to_scale)}\")\n",
    "print(f\"\\nNumerical columns that will be scaled:\")\n",
    "for col_name in numerical_columns_to_scale[:30]:  # Show first 30\n",
    "    print(f\"  ‚Ä¢ {col_name}\")\n",
    "if len(numerical_columns_to_scale) > 30:\n",
    "    print(f\"  ... and {len(numerical_columns_to_scale) - 30} more\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ### Step 2: Convert to Pandas for Scaling\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\nConverting to Pandas (this may take 2-3 minutes)...\")\n",
    "df_pandas = df_fixed.toPandas()\n",
    "\n",
    "print(f\"‚úÖ Converted to Pandas: {df_pandas.shape[0]:,} rows √ó {df_pandas.shape[1]} columns\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ### Step 3: Apply StandardScaler\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\nApplying StandardScaler to numerical columns...\")\n",
    "\n",
    "# Create a copy of the dataframe\n",
    "df_scaled = df_pandas.copy()\n",
    "\n",
    "# Initialize StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale only the numerical columns\n",
    "df_scaled[numerical_columns_to_scale] = scaler.fit_transform(\n",
    "    df_pandas[numerical_columns_to_scale]\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ StandardScaler applied to {len(numerical_columns_to_scale)} columns\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ### Step 4: Verify Scaling\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n=== Verification: Scaled vs Original ===\\n\")\n",
    "\n",
    "# Check a few example columns\n",
    "example_cols = ['AMT_INCOME_TOTAL', 'AMT_CREDIT', 'DAYS_BIRTH', 'DAYS_EMPLOYED']\n",
    "\n",
    "for col_name in example_cols:\n",
    "    if col_name in numerical_columns_to_scale:\n",
    "        print(f\"{col_name}:\")\n",
    "        print(f\"  Original - Mean: {df_pandas[col_name].mean():.2f}, Std: {df_pandas[col_name].std():.2f}\")\n",
    "        print(f\"  Scaled   - Mean: {df_scaled[col_name].mean():.2e}, Std: {df_scaled[col_name].std():.2f}\")\n",
    "        print(f\"  Original range: [{df_pandas[col_name].min():.2f}, {df_pandas[col_name].max():.2f}]\")\n",
    "        print(f\"  Scaled range:   [{df_scaled[col_name].min():.2f}, {df_scaled[col_name].max():.2f}]\")\n",
    "        print()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Verify binary columns were NOT scaled\n",
    "print(\"=== Binary Columns (Should NOT be scaled) ===\")\n",
    "example_binary = binary_columns[:5]\n",
    "\n",
    "for col_name in example_binary:\n",
    "    unique_values = sorted(df_scaled[col_name].unique())\n",
    "    print(f\"{col_name}: unique values = {unique_values}\")\n",
    "\n",
    "print(\"\\n‚úÖ Binary columns preserved (still 0 and 1)\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Verify TARGET column was not scaled\n",
    "print(\"\\n=== Target Variable ===\")\n",
    "print(f\"TARGET unique values: {sorted(df_scaled['TARGET'].unique())}\")\n",
    "print(f\"TARGET distribution:\")\n",
    "print(df_scaled['TARGET'].value_counts())\n",
    "print(\"\\n‚úÖ TARGET column preserved\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ### Step 5: Summary Statistics\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n=== Summary Statistics for Scaled Numerical Columns ===\\n\")\n",
    "\n",
    "# Get stats for scaled numerical columns\n",
    "scaled_stats = df_scaled[numerical_columns_to_scale].describe()\n",
    "print(scaled_stats)\n",
    "\n",
    "print(\"\\nKey observations:\")\n",
    "print(f\"‚Ä¢ Mean of scaled columns should be ~0: {scaled_stats.loc['mean'].abs().mean():.6f}\")\n",
    "print(f\"‚Ä¢ Std of scaled columns should be ~1: {scaled_stats.loc['std'].mean():.6f}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ### Step 6: Convert Back to Spark and Save\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\nConverting back to Spark DataFrame...\")\n",
    "\n",
    "# Convert back to Spark\n",
    "df_scaled_spark = spark.createDataFrame(df_scaled)\n",
    "\n",
    "print(f\"‚úÖ Converted back to Spark\")\n",
    "\n",
    "# Save to table\n",
    "print(\"Saving to table...\")\n",
    "df_scaled_spark.write.mode(\"overwrite\").saveAsTable(\"credit_risk.application_train_scaled\")\n",
    "\n",
    "print(\"‚úÖ Scaled dataset saved as 'credit_risk.application_train_scaled'\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# MAGIC %md\n",
    "# MAGIC ### Step 7: Final Summary\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SCALING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nüìä Dataset Summary:\")\n",
    "print(f\"  ‚Ä¢ Total rows: {df_scaled.shape[0]:,}\")\n",
    "print(f\"  ‚Ä¢ Total columns: {df_scaled.shape[1]}\")\n",
    "print(f\"  ‚Ä¢ Columns scaled: {len(numerical_columns_to_scale)}\")\n",
    "print(f\"  ‚Ä¢ Binary columns (not scaled): {len(binary_columns)}\")\n",
    "print(f\"  ‚Ä¢ Excluded columns: {len(columns_to_exclude)}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Scaling Method: StandardScaler\")\n",
    "print(f\"  ‚Ä¢ Formula: (x - mean) / std\")\n",
    "print(f\"  ‚Ä¢ Result: Mean ‚âà 0, Std ‚âà 1\")\n",
    "\n",
    "print(f\"\\nüìÅ Saved as: credit_risk.application_train_scaled\")\n",
    "\n",
    "print(f\"\\nüéâ Dataset is now ready for:\")\n",
    "print(f\"  ‚Ä¢ Correlation analysis\")\n",
    "print(f\"  ‚Ä¢ Feature selection\")\n",
    "print(f\"  ‚Ä¢ Logistic regression modeling\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Display sample of scaled data\n",
    "print(\"\\n=== Sample of Scaled Data ===\")\n",
    "display(df_scaled.head(10))\n",
    "\n",
    "# COMMAND ----------"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data_normalization",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
